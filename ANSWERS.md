## 1. **Technical Choices:** What influenced your decision on the specific tools and models used?
**For PDF parsing:** PDFs are easy to read by humans but not computers, therefore it is a challenging task in itself. I decided to go for text extraction rather than OCR solution  because of
1) computational power - image-to-text conversion can be computationally intensive even for inference and thus GPU might be required for only pdf parsing, not to mention the entity extraction
2) complexity - if we applied a pretrained ML / OCR system we would need to implement additional validation step; once a model converts PDF to text, we would need to ensure it converted it correctly (e.g. it didnt skip any parts, mixed similar letters like Ã¶o0). It would be a +4hr challenge in itself
That being said, there are huge benefits of using OCR approaches as we would more likely get rid of noise sections (e.g. annotations, references), we would get semantic context (eg which section refers to each). The solution would also be more robust (as we could also analyse scans of pdfs) but implementation of that would be too complex. Of course one could use existing tools such as textrct That's why I decided to go with simple pdf text extraction

**For NLP:** I decided to go with spacy & its derivative library sciSpacy, developed by AllenAI. Spacy is commonly used for NLP tasks and NER tasks however one of the main reasons why I chose it is because spacy provides NLP pipelines with pretrained models for NLP tasks such as NER. Whats important about those tasks is that they contain pre-processing pipeline involving tokenization, tagger, parser, lemmatizer etc) for typical NLP tasks, therefore preprocessing should be taken care of. While this is not an approach I am fully satisfied with, and I could use more fine-grained libraries like NLTK, text data preprocessing is very time-consuming and I simply wouldnt have time to attend remaining tasks. 

Alternatively I was considering using a hugging face NER models however the transformers hub seemed down when I was trying to access one of the models, meaning I couldnt even explore it. Furthermore for these models I would probably need to preprocess the text myself

**For API:** I chose Gradio as I am most familiar with it, it's quick and easy to implement and offers simple UI. Although it's not a fully scalalbe and flexible API solution (in fact its more of a web app that can be also utilized as an API). Furthermore it can be also integrated with FastAPI so certain level of scalability of the application can be preserved. FastAPI or Flask can be definitely more secure and scalable and offer more fine-grained control but with the task having so many aspects, it was time that counted the most for me.

## 2. **Entity Contextualization:** How did you approach the problem of providing context for each identified entity?
Although my approach is far from best, it is the safest & fastest to implement solution for contextualization. We could potentially use PoS tagging and connect the entities linguistically / grammatically, but that would again require some time-consuming validation. As the context of an entity is usually within its vicinity (nearby tokens usually define tokens, of course not always as thats why we have attention), my solution should give at least partially correct results. However I need to note that without examples of extracted entities and associated contexts (one example was provided but it was not very helpful as it didnt give broader context, didnt show what exacty entity is it; it seemed to be there mostly for API specification purposes), we are confined to more 'generic' solution.


## 3. **Error Handling:** Can you describe how your API handles potential errors?_
I did not have much time for error handling.  If more time was available, I would have written proper unit tests for the code allowing for better error handling, but with exploration of the test pdfs, examining different parsing and NER methods as well as app/API development, TDD is not really viable. That being said though, gradio allows for flagging errors by users, which is helpful from MLOps perspective; the live app on hugging face will have logs recording errors.

## 4. **Challenges and Learnings:** What were the top challenges faced, and what did you learn from them?_
* Lack of model specificity - while it was mentioned that high entity extraction accuracy would be valued, how can we assess it if no proper target entities are provided to the supplied papers? 
* PDF noise - with the pdf-parsing strategy I selected, the amount of noise, encodings etc contained within each PDF was definitely a great challenge. PDFs are naturally 'noisy', despite looking nice to human eye, due to the way text is encoded, varying layouts, watermarks etc. It shows that like in other Data / ML problems, it is data cleaning that requires most attention and most attention should be focused on this. It taught me that OCR approach would probably be more suitable, robust and contextual and I probably should have looked into this instead (although OCR-based solution would take much more time than 4hrs)
* generecity of selected models - although I was expecting the models to not perform perfectly when they are taken without any fine-tuning, I was surprised and challenged to find that all explored NERs were often extracting uninformatives entities and labelling them - the scale of false postiives was really astounding. It showed me again the importance of data preparation.


## 5. **Improvement Propositions:** Given more time, what improvements or additional features would you consider adding?_
* Unit & end-to-end testing - crucial in both ML and software development. Normally I use pytest and conduct testing in parallel to the development but in this timeframe it is not possible
* Generate proper data preprocessing pipeline - partially related to PDF parsing but we should preprocess extracted texts to remove watermarks, only keep relevant sections etc.
* Generate proper postprocessing pipeline - once entities and contexts are extracted, we should check for redundant information
* Relationship Extraction - once entities are extracted we could link them and convert them to graphs. This could reveal new hidden relationships within a paper
* Examine OCR models - as mentioned earlier, PDFs tend to be very noisy and OCRs might provide higher quality data. Would be definitely beneficial to examine
* Fine-tune NER model for our ownr purpose (if more data and targets were available) - fine-tuning for our specific purpose is almost always better than using solely pre-trained and rather generic approaches
* Deployment of the app & Scaling using other API tools and docker - while gradio is fast and easy to use, it is definitely not scalalbe and as optimizable as Flask or other API tools, it might also lack security required on enterprise level.
